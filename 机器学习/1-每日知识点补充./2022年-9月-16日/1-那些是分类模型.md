在分类任务中，我们常常会从历史的观测数据中总结规律，用于未知数据的预测。总结规律的过程则被看为建模的过程。




1、KNN k近邻算法：【监督--分类算法】
        
        如果一个数据在特征空间中最相邻的k个数据中的大多数属于某一个类别，则该样本也属于这个类别（类似投票），并具有这个类别上样本的特性。
        
        
        通俗的说，如果给定一组数据集，与一个待【分类的】数据，你需要将这个数据与数据集中的每个个体进行比较，然后选择出K个最接近的；
        
        如果在这个k个数据中，有大部分的数据属于同一类，那么将这个数据划分为该类
        
        【k的最优值，需要通过【实验】来确定。从k=1开始，使用检验数据集来估计分类器的错误率。
        重复该过程，每次k增加1，允许增加一个近邻，选取产生最小错误率的k。
        一般而言，训练数据集越多，k的值越大，使得分类预测可以基于训练数据集的较大比例。
        在应用中，一般选择较小k并且k是奇数。通常采用交叉验证的方法来选取合适的k值。】
        
        sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, 
             p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)
        参数注释：

        1，n_neighbors，临近的节点数量，默认值是5

        2，weights，权重，默认值是uniform，

            uniform：表示每个数据点的权重是相同的；
            distance：离一个簇中心越近的点，权重越高；
            callable：用户定义的函数，用于表示每个数据点的权重

        3，algorithm，算法

            auto：根据值选择最合适的算法
            ball_tree：使用BallTree
            kd_tree：KDTree
            brute：使用Brute-Force查找

        4，p，metric，metric_paras，距离度量

            p参数用于设置Minkowski 距离的Power参数，当p=1时，等价于manhattan距离；当p=2等价于euclidean距离，当p>2时，就是Minkowski 距离。
            metric参数：设置计算距离的方法
            metric_paras：传递给计算距离方法的参数


 2、Naïve Bayesian 朴素贝叶斯---【分类】
 
        
        

 3、Logistic Regression逻辑回归 ----【类】
 
 
 
 
 
