【重点】

决策树是一种用于【分类和回归任务】的【非参数监督学习】算法。 
它是一种分层树形结构，由根节点、分支、内部节点和叶节点组成。

优点：【计算复杂度不高】，输出结果易于理解，对【中间值的缺失不敏感】，【可以处理不相关特征数据】
缺点：可能会产生过度匹配的问题；
适用数据类型： 数值型和标称型数据（数据为“是” 或者 “否”）


几种决策树算法：

1、ID3算法 ------ 基于【信息增益】选择特征；

（1）计算信息熵：
  计算方式：假定D是样本集合，那么第k类样本的所占比例就是Pk，（比例就是第k类样本数占总数的百分比）-----计算第k类在整个样本集上的信息熵；
      依据以下公式计算信息熵---每个类的信息熵
      
 ![图片](https://user-images.githubusercontent.com/38878365/180675725-b074508b-a9bc-40af-b7a5-718e18269baa.png)

（2）信息增益 
    对每一个属性去求解熵，每个属性又分为多个类别，每个类别占总数的的比例为D^v/D,每个属性的类别的信息熵为Ent（D^v）

    那么最后的信息增益表达为：
    
 ![图片](https://user-images.githubusercontent.com/38878365/180676577-c2747497-8847-4c9f-8e67-b542e4bda188.png)

ID3特点： 可以用于划分标称型数据集，没有剪枝的过程，为了去除多读数据匹配的问题，可以通过裁剪和合并相邻的无法产生大量信息增益的叶子节点；
ID3的缺点： 它往往选择【信息增益大】的属性作为优先划分的分裂属性； 如果在训练过程中，某个属性的取值越多，那么它很有可能被用来作为划分属性




2、改进方法 C4.5

![图片](https://user-images.githubusercontent.com/38878365/180676869-26e5e140-55c8-44ae-81b3-c4b2d7c45591.png)

以下几方面对ID3算法进行了改进：

克服了用信息增益选择属性时偏向选择取值多的属性的不足；
在树构造过程中进行剪枝；
能够完成对连续属性的离散化处理；
能够对不完整数据进行处理。


优点： 易于理解、准确度高；
缺点：效率低下、在树构造过程中，需要对数据集进行多次的顺序扫描和排序。

它和ID3都是基于贪心性质
