【重点】

决策树是一种用于【分类和回归任务】的【非参数监督学习】算法。 
核心是在决策树各个节点上应用 信息增益 准则选择特征，递归的构建决策树。
具体方法是：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归的调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。
ID3相当于用极大似然法进行概率模型的选择。
使用 二元切分法 则易于对树构建过程中进行调整以处理连续型特征。
具体的处理方法是: 如果特征值大于给定值就走左子树，否则走右子树。另外二元切分法也节省了树的构建时间。


优点：【计算复杂度不高】，输出结果易于理解，对【中间值的缺失不敏感】，【可以处理不相关特征数据】
缺点：可能会产生过度匹配的问题；
适用数据类型： 数值型和标称型数据（数据为“是” 或者 “否”）


几种决策树算法：

1、ID3算法 ------ 基于【信息增益】选择特征；

（1）计算信息熵：
  计算方式：假定D是样本集合，那么第k类样本的所占比例就是Pk，（比例就是第k类样本数占总数的百分比）-----计算第k类在整个样本集上的信息熵；
      依据以下公式计算信息熵---每个类的信息熵
      
 ![图片](https://user-images.githubusercontent.com/38878365/180675725-b074508b-a9bc-40af-b7a5-718e18269baa.png)

（2）信息增益 
    对每一个属性去求解熵，每个属性又分为多个类别，每个类别占总数的的比例为D^v/D,每个属性的类别的信息熵为Ent（D^v）

    那么最后的信息增益表达为：
    
 ![图片](https://user-images.githubusercontent.com/38878365/180676577-c2747497-8847-4c9f-8e67-b542e4bda188.png)

ID3特点： 可以用于划分标称型数据集，没有剪枝的过程，为了去除多读数据匹配的问题，可以通过裁剪和合并相邻的无法产生大量信息增益的叶子节点；
ID3的缺点： 它往往选择【信息增益大】的属性作为优先划分的分裂属性； 如果在训练过程中，某个属性的取值越多，那么它很有可能被用来作为划分属性




2、改进方法 C4.5
算法用信息增益率选择特征，在树的构造过程中会进行剪枝操作优化，能够自动完成对连续属性的离散化处理；在选择分割属性时选择信息增益率最大的属性。

![图片](https://user-images.githubusercontent.com/38878365/180676869-26e5e140-55c8-44ae-81b3-c4b2d7c45591.png)

以下几方面对ID3算法进行了改进：

克服了用信息增益选择属性时偏向选择取值多的属性的不足；
在树构造过程中进行剪枝；
能够完成对连续属性的离散化处理；
能够对不完整数据进行处理。


优点： 易于理解、准确度高；
缺点：效率低下、在树构造过程中，需要对数据集进行多次的顺序扫描和排序。

为什么需要适用信息增益比：
在使用信息增益的时候，如果某个特征有很多取值，使用这个取值多的特征会的大的信息增益，这个问题是出现很多分支，将数据划分更细，模型复杂度高，出现过拟合的机率更大。【使用信息增益比就是为了解决偏向于选择取值较多的特征的问题】. 使用信息增益比【对取值多的特征加上的惩罚】，对这个问题进行了校正.




3、CART（基于基尼指数）***
CART二分每个特征（包括标签特征以及连续特征），经过最优二分特征及其最优
二分特征值的选择、切分，二叉树生成，剪枝来实现CART算法。对于回归CART树
选择误差平方和准则、对于分类CART树选择基尼系数准则进行特征选择，并递归
调用构建二叉树过程生成CART树。

![图片](https://user-images.githubusercontent.com/38878365/180678977-9ccb8f81-87cb-43d6-a4ac-b3310a75469f.png)

问题：基尼指数和信息熵都表示数据不确定性，为什么CART使用基尼指数？？

信息熵0, logK都是值越大，数据的不确定性越大. 信息熵需要计算对数，计算量大；信息熵是可以处理多个类别，基尼指数就是针对两个类计算的，由于CART树是一个二叉树，每次都是选择yes or no进行划分，从这个角度也是应该选择简单的基尼指数进行计算.

问题：基尼系数(Gini)存在的问题?
基尼指数偏向于【多值属性【;当类数较大时，基尼指数求解比较困难;基尼指数倾向于支持在两个分区中生成大小相同的测试。

![图片](https://user-images.githubusercontent.com/38878365/180679139-0ee7f6c6-0f26-4337-b286-841d82d2eda4.png)

![图片](https://user-images.githubusercontent.com/38878365/180679185-d2283170-28c1-45e0-8c8c-13af0fa06d8d.png)

4、剪枝
通过降低决策树的复杂度来避免过拟合的过程---称之为剪枝

****************************************************************************************************************************************
决策树量化纯度：
判断数据集“纯”的指标有三个：Gini指数、熵、错误率

决策树的数据split原理或者流程？
1.将所有样本看做一个节点
2.根据纯度量化指标.计算每一个特征的’纯度’,根据最不’纯’的特征进行数据划分
3.重复上述步骤,知道每一个叶子节点都足够的’纯’或者达到停止条件
背诵：按照基尼指数、信息增益来选择特征，保证划分后纯度尽可能高。


构造决策树的步骤？
1.特征选择
2.决策树的生成（包含预剪枝）  ---- 只考虑局部最优
3.决策树的剪枝（后剪枝）      ---- 只考虑全局最优


决策树算法中如何避免过拟合和欠拟合？‍
过拟合:选择能够反映业务逻辑的训练集去产生决策树;
剪枝操作(前置剪枝和后置剪枝); K折交叉验证(K-fold CV)
欠拟合:增加树的深度,RF



决策树怎么剪枝？
决策树的优点：
1.决策树模型可读性好，具有描述性，有助于人工分析；
2.效率高，决策树只需要一次性构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
决策树的缺点：
1.即使做了预剪枝，它也经常会过拟合，泛化性能很差。
2.对中间值的缺失敏感；
3.ID3算法计算信息增益时结果偏向数值比较多的特征。




如果特征很多，决策树中最后没有用到的特征一定是无用吗？
不是无用的，从两个角度考虑，一是特征替代性，如果可以已经使用的特征A和特征B可以提点特征C，特征C可能就没有被使用，但是如果把特征C单独拿出来进行训练，依然有效. 其二，决策树的每一条路径就是计算条件概率的条件，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据.


为什么使用贪心和其发生搜索建立决策树，为什么不直接使用暴力搜索建立最优的决策树？
决策树目的是构建一个与训练数据拟合很好，并且复杂度小的决策树. 因为从所有可能的决策树中直接选择最优的决策树是NP完全问题，在使用中一般使用启发式方法学习相对最优的决策树.



决策树怎么做回归？
给回归定义一个损失函数，比如 L2 损失，可以把分叉结果量化；最终的输出值，是分支下的样本均值。 [切分点选择：最小二乘法]; [输出值：单元内均值].


![图片](https://user-images.githubusercontent.com/38878365/180679724-a28d8a5b-a25a-4daf-9904-83e617b13eb5.png)

![图片](https://user-images.githubusercontent.com/38878365/180679802-3f59e090-c653-4623-91d2-49d444278bd4.png)


![图片](https://user-images.githubusercontent.com/38878365/180679838-893392ab-6b97-41c6-9d01-b4c45214365c.png)

![图片](https://user-images.githubusercontent.com/38878365/180679896-bc74d12a-fba4-499d-a4c6-05ad784eba5c.png)

