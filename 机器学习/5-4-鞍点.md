在早期的神经网络研究中，人们普遍认为，神经网络的优化问题是因为较大的神经网络中包含许多局部极小值（local minima），使得算法陷入到其中某些点，然而，2014年，
一篇论文《Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》，提出【高维非凸优化问题之所以困难，是因为存在大量的鞍点；


什么是鞍点？

数学定义：
【一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得算法陷入其中很难脱离出来，因为梯度在所有维度上接近于零。】

![图片](https://user-images.githubusercontent.com/38878365/180629894-b17e149a-ca34-4080-b8e5-5ebcd5465fcf.png)


![图片](https://user-images.githubusercontent.com/38878365/180630012-bed7f57a-4f58-41b5-ad53-079668fb7640.png)



鞍点和局部值点的区别是：

相似点：在该处的一阶导数都等于0，但是不同点在于鞍点附近的Hessian矩阵是不定的（行列式小于0），而局部极值附近的hessian矩阵是正定的。

鞍点的一个充分条件是：函数在一阶导数处的【黑塞矩阵】为不定矩阵

【Hessian矩阵：】
是一个【多元函数】的二阶【偏导数】构成的方阵，描述了函数的局部曲率。可用
![图片](https://user-images.githubusercontent.com/38878365/180630172-987f8c2c-8f8e-4ced-9e09-119b46cf0eba.png)


正定矩阵:
1个nxn的实对称矩阵是正定的，当且仅当对于所有的非0实系数向量z，都有zTMZ>0

![图片](https://user-images.githubusercontent.com/38878365/180630498-b20f4cf9-2e50-43c6-9bcc-526348c4834f.png)

负定矩阵：
1、特征值全部小于0，
2、A的k阶顺序主子式*（-1）^k>0
与正定矩阵相对应的，一个n×n的埃尔米特矩阵(复数共轭对称矩阵)M是负定矩阵当且仅当对所有不为零的x \in \mathbb{R}^n（或x \in \mathbb{C}^n），都有：



半正定矩阵：所有特征值为非负。
![图片](https://user-images.githubusercontent.com/38878365/180630507-64dacfb0-4c25-43c3-aaec-6e8d9f72575d.png)


半负定矩阵：所有特征值为非正

不定矩阵：特征值有正有负
