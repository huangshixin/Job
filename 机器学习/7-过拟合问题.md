什么是过拟合？---它是一种现象，还是一种结果？


过拟合是一种现象，从【损失的角度】解释：
  在模型的训练过程在进行时候，模型的复杂度增加，在训练集上的的【损失error】不断逐渐减少，但是在验证集上的而损失却不断增大；因此我们称之为模型过拟合了训练集，导致泛化性能差；
  
  ![图片](https://user-images.githubusercontent.com/38878365/180628499-c46b8ec6-6c4d-4b48-9849-946357a74c3e.png)

传统的函数拟合问题，一把是通过物理数学等推导出的一个含参数的模型，模型的复杂度是确定的，没有多余的能力拟合噪声。而机器学习算法的复杂度更高，
一般都远高于具体问题的复杂度，数据量不足以支撑庞大的模型；



2、出现这种情况的原因；可能是数据太少，



如何降低过拟合？


1、正则化
L2正则化：目标函数中增加所有权重w参数的平方和，逼迫所有w尽可能地趋向于0但不为0
![图片](https://user-images.githubusercontent.com/38878365/180628840-b4275ce3-6b2f-4fd9-a1e5-a39371ee5ed6.png)

L1正则化：
![图片](https://user-images.githubusercontent.com/38878365/180628855-5eed2455-cb72-4841-8108-d46e953a6e76.png)



2、随机失活 dropout
L1 和 L2正则化是通过修改代价函数来实现地，而Dropout则是通过修改神经网络本身来实现地；

在训练过程中，让神经元以超参数p地概率被失活，每个w因此随机参与，使得任意w都不是不可或缺地；

运用了dropout地训练过程，相当于训练了很多半数隐层单元地神经网络，每一个这样地半数网络，都可以给出一个分类结果，


3、逐层、批归一化 normalization
每一层地【输出】都做一次归一化，使得下一层地输入接近高斯分布，避免了在学习过程中训练数据地分布各不相同，也不同于测试数据地分布，因此提高了泛化能力。


4、early stopping

![图片](https://user-images.githubusercontent.com/38878365/180629083-6b236a85-02f5-4fbd-bc15-cb157caa503b.png)


5、数据集扩增

提供更多地训练数据，意味着可以使用更深地网络，训练出更好地模型。

![图片](https://user-images.githubusercontent.com/38878365/180629138-d459b7a2-eaa9-4ca4-9151-837aa498ea92.png)

